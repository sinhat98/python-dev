{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb8118a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# symbol.txtファイルを読み込んで、_SPECIALSに記号を追加\n",
    "# https://github.com/reazon-research/ReazonSpeech/blob/master/pkg/_v1/src/data/symbol.txt\n",
    "with open('symbol.txt', 'r') as f:\n",
    "    _SPECIALS = {ord(c.rstrip(\"\\n\")): \"\" for c in f.readlines()}\n",
    "\n",
    "_HAN2ZEN = str.maketrans(\n",
    "    \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\",\n",
    "    \"ａｂｃｄｅｆｇｈｉｊｋｌｍｎｏｐｑｒｓｔｕｖｗｘｙｚＡＢＣＤＥＦＧＨＩＪＫＬＭＮＯＰＱＲＳＴＵＶＷＸＹＺ０１２３４５６７８９\")\n",
    "\n",
    "def normalize(text):\n",
    "    \"\"\"Trim non-phonatory symbols in the text\n",
    "\n",
    "    Args:\n",
    "        text(str): A string to process\n",
    "\n",
    "    Returns:\n",
    "        A normalized string\n",
    "    \"\"\"\n",
    "    text = text.replace('<sos/eos>', '')\n",
    "    text = text.replace('<unk>', '')\n",
    "    return text.translate(_SPECIALS).translate(_HAN2ZEN)\n",
    "\n",
    "\n",
    "# cerを計測する関数を定義\n",
    "import editdistance\n",
    "\n",
    "def calculate_cer(reference_texts, predicted_texts):\n",
    "    total_errors = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    for ref, pred in zip(reference_texts, predicted_texts):\n",
    "        # Levenshtein距離を使ってエディット距離を計算する\n",
    "        levenshtein_distance = editdistance.eval(ref, normalize(pred))\n",
    "        total_errors += levenshtein_distance\n",
    "        total_chars += len(ref)\n",
    "    \n",
    "    # CERを計算する\n",
    "    cer = total_errors / total_chars\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d00651c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのダウンロード\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('/root/datadrive/TEDxJP-10K_v1.1')\n",
    "wav_dir = data_dir / 'test_dump'\n",
    "id2text = pd.read_csv(data_dir / 'text', sep=' ', header=None, names=['id', 'text']).set_index('id')['text'].to_dict()\n",
    "file_id_list = list(id2text.keys())\n",
    "paths2audio_files = [wav_dir / f'{file_id}.wav' for file_id in file_id_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c4dc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       /root/datadrive/TEDxJP-10K_v1.1/test_dump/-6K2...\n",
       "1       /root/datadrive/TEDxJP-10K_v1.1/test_dump/-6K2...\n",
       "2       /root/datadrive/TEDxJP-10K_v1.1/test_dump/-6K2...\n",
       "3       /root/datadrive/TEDxJP-10K_v1.1/test_dump/-6K2...\n",
       "4       /root/datadrive/TEDxJP-10K_v1.1/test_dump/-6K2...\n",
       "                              ...                        \n",
       "9911    /root/datadrive/TEDxJP-10K_v1.1/test_dump/zwW9...\n",
       "9912    /root/datadrive/TEDxJP-10K_v1.1/test_dump/zwW9...\n",
       "9913    /root/datadrive/TEDxJP-10K_v1.1/test_dump/zwW9...\n",
       "9914    /root/datadrive/TEDxJP-10K_v1.1/test_dump/zwW9...\n",
       "9915    /root/datadrive/TEDxJP-10K_v1.1/test_dump/zwW9...\n",
       "Name: id, Length: 9916, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "df = pd.read_csv(data_dir / 'text', sep=' ', header=None, names=['id', 'text'])\n",
    "\n",
    "def calc_duration(file_id):\n",
    "    wav_path = wav_dir / f'{file_id}.wav'\n",
    "    audio, sr = sf.read(wav_path)\n",
    "    return len(audio) / sr\n",
    "\n",
    "df['id'].map(lambda x: wav_dir / f'{x}.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f77e167d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.820502934027777"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['duration'] = df['id'].map(calc_duration)\n",
    "df['duration'].sum() / 3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3dd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "for path in paths2audio_files:\n",
    "    data, sr = sf.read(path)\n",
    "    duration = len(data) / sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8539f382",
   "metadata": {},
   "source": [
    "# ReazonSpeech (NeMo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d87a7",
   "metadata": {},
   "source": [
    "```shell\n",
    "!pip install Cython\n",
    "!pip install nemo_toolkit['asr']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af0db196-11e8-492f-8cd4-0015fba0c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-31 12:52:14 mixins:172] Tokenizer SentencePieceTokenizer initialized with 3000 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-31 12:52:15 modelPT:165] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: dataset/train.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 32\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    max_duration: 30\n",
      "    min_duration: 0.1\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2024-05-31 12:52:15 modelPT:172] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: dataset/valid.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2024-05-31 12:52:15 modelPT:178] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: dataset/test.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-05-31 12:52:15 features:289] PADDING: 0\n",
      "[NeMo I 2024-05-31 12:52:18 rnnt_models:217] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2024-05-31 12:52:20 save_restore_connector:249] Model EncDecRNNTBPEModel was successfully restored from /root/datadrive/reazonspeech-nemo-v2/reazonspeech-nemo-v2.nemo.\n"
     ]
    }
   ],
   "source": [
    "from nemo.collections.asr.models import EncDecRNNTBPEModel\n",
    "model = EncDecRNNTBPEModel.restore_from('/root/datadrive/reazonspeech-nemo-v2/reazonspeech-nemo-v2.nemo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b733b6e9-f4b2-4649-a9a9-0889c11629b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-05-30 23:39:21 nemo_logging:349] /tmp/ipykernel_3322/2092515057.py:10: DeprecationWarning: Sampling from a set deprecated\n",
      "    since Python 3.9 and will be removed in a subsequent version.\n",
      "      file_id_list = random.sample(id2text.keys(), 10)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('/root/datadrive/TEDxJP-10K_v1.1')\n",
    "wav_dir = data_dir / 'test_dump'\n",
    "id2text = pd.read_csv(data_dir / 'text', sep=' ', header=None, names=['id', 'text']).set_index('id')['text'].to_dict()\n",
    "\n",
    "# 10個サンプルする\n",
    "file_id_list = random.sample(id2text.keys(), 10)\n",
    "\n",
    "paths2audio_files = [str(wav_dir / f'{file_id}.wav') for file_id in file_id_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ce6b2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a3e25cf7fe49fbbc2b2590d4421cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transcribing:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Beam search progress:: 100%|██████████| 1/1 [00:00<00:00, 12.37sample/s]\n",
      "Beam search progress:: 100%|██████████| 1/1 [00:00<00:00, 20.36sample/s]\n",
      "Beam search progress:: 100%|██████████| 1/1 [00:00<00:00, 16.29sample/s]\n",
      "Beam search progress:: 100%|██████████| 1/1 [00:00<00:00, 24.62sample/s]\n",
      "Beam search progress:: 100%|██████████| 1/1 [00:00<00:00, 37.98sample/s]\n",
      "Beam search progress:: 100%|██████████| 1/1 [00:00<00:00,  8.63sample/s]\n",
      "Beam search progress:: 100%|██████████| 1/1 [00:00<00:00,  8.57sample/s]\n",
      "Beam search progress:: 100%|██████████| 1/1 [00:00<00:00, 11.63sample/s]\n",
      "Beam search progress:: 100%|██████████| 1/1 [00:00<00:00, 15.75sample/s]\n",
      "Beam search progress:: 100%|██████████| 1/1 [00:00<00:00, 42.66sample/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcribe time: 1.32s\n",
      "['使い古された技術を使った、',\n",
      " '一つの国家に変わってしまうと。',\n",
      " '俺しゃべり下手やしな。',\n",
      " 'コミュニケーションも下手で。',\n",
      " '一方で、',\n",
      " '本当にだんだん体が動かなくなって。',\n",
      " 'それと同時に、貧しさや差別。',\n",
      " '学校にはですね机と椅子がありませんでした。',\n",
      " 'な部分があったりとかって結構しますね。',\n",
      " '登ったり。']\n",
      "0.29605263157894735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import time\n",
    "tic = time.perf_counter()\n",
    "asr_text = model.transcribe(paths2audio_files=paths2audio_files, batch_size=1, return_hypotheses=False)[0]\n",
    "toc = time.perf_counter() - tic\n",
    "print(f\"transcribe time: {toc:.2f}s\")\n",
    "gt_text = [id2text[file_id] for file_id in file_id_list]\n",
    "cer = calculate_cer(gt_text, asr_text)\n",
    "pprint(asr_text)\n",
    "print(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d4d31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27e11250",
   "metadata": {},
   "source": [
    "# Kotoba-Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0adb541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcribe time: 2.15s\n",
      "['使い古された技術を使った',\n",
      " '一つの国家に変えてしまうと',\n",
      " 'でもな俺しゃべり下手やしな',\n",
      " 'コミュニケーションも下手で',\n",
      " 'でも一方で',\n",
      " 'そうしても本当にもうだんだん体がない動かなくなって',\n",
      " 'それと同時に貧しさや差別',\n",
      " 'この学校には机と椅子がありませんでした',\n",
      " '根拠に曖昧な部分があったりとかって結構しまして',\n",
      " '山に登ったり']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19078947368421054"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# config\n",
    "model_id = \"kotoba-tech/kotoba-whisper-v1.0\"\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_kwargs = {\"attn_implementation\": \"sdpa\"} if torch.cuda.is_available() else {}\n",
    "generate_kwargs = {\"language\": \"japanese\", \"task\": \"transcribe\"}\n",
    "\n",
    "# load model\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    model_kwargs=model_kwargs\n",
    ")\n",
    "\n",
    "asr_text = []\n",
    "tic = time.perf_counter()\n",
    "for sample in paths2audio_files:\n",
    "    # run inference\n",
    "    result = pipe(sample, generate_kwargs=generate_kwargs)\n",
    "    asr_text.append(result[\"text\"])\n",
    "toc = time.perf_counter() - tic\n",
    "print(f\"transcribe time: {toc:.2f}s\")\n",
    "pprint(asr_text)\n",
    "cer = calculate_cer(gt_text, asr_text)\n",
    "cer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2be56f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "690c8202b1f84248a5152807a524bfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957c23ecd03c4dacaf19669bf1c7e74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.03G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08fe8418ad2443cb73877b1c361db4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7183eb13d2284b85850f05bc8a6c576a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d6613943d845d3b042fbdb86c93ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e30a7f06fba4d49bbe699e53e3144e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff63de04467743fb83cc05896a473a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228cb21ebecd4e22b66793e45712c6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6906bda2714422894202495b0c85ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12983d0f50bb4eafb4d981016d80660d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa273cd27d53484dbf9c95fea3c99395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# config\n",
    "model_id = \"kotoba-tech/kotoba-whisper-v1.0\"\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_kwargs = {\"attn_implementation\": \"sdpa\"} if torch.cuda.is_available() else {}\n",
    "generate_kwargs = {\"language\": \"japanese\", \"task\": \"transcribe\"}\n",
    "\n",
    "# load model\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    model_kwargs=model_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833cefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_kwargs = {\"language\": \"japanese\", \"task\": \"transcribe\", \"return_timestamp\": True}\n",
    "sample = paths2audio_files[0]\n",
    "pipe(sample, generate_kwargs=generate_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc7c99",
   "metadata": {},
   "source": [
    "# Nue-ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "149d1157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting deepspeed\n",
      "  Downloading deepspeed-0.14.2.tar.gz (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting hjson\n",
      "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 KB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ninja\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 KB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deepspeed) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed) (24.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed) (5.9.8)\n",
      "Collecting py-cpuinfo\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-2.7.2-py3-none-any.whl (409 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pynvml\n",
      "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from deepspeed) (2.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed) (4.12.0)\n",
      "Collecting pydantic-core==2.18.3\n",
      "  Downloading pydantic_core-2.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.1.3.1)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (2.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.1.105)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (3.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (11.4.5.107)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->deepspeed) (1.12.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->deepspeed) (12.5.40)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->deepspeed) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->deepspeed) (1.3.0)\n",
      "Building wheels for collected packages: deepspeed\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.14.2-py3-none-any.whl size=1432249 sha256=46ed6a9f7c3afd7f27d4d7a1da0f923cb4cab2bc697f17ddff1ab82c87a93d0e\n",
      "  Stored in directory: /root/.cache/pip/wheels/ea/7c/43/bed44d8414c099ff962b754f425f7ff77cc623cc8a98e0da70\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, ninja, hjson, pynvml, pydantic-core, annotated-types, pydantic, deepspeed\n",
      "Successfully installed annotated-types-0.7.0 deepspeed-0.14.2 hjson-3.1.0 ninja-1.11.1.1 py-cpuinfo-9.0.0 pydantic-2.7.2 pydantic-core-2.18.3 pynvml-11.5.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/rinnakk/nue-asr.git\n",
    "!pip install deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc34ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at rinna/nue-asr were not used when initializing NueASRModel: ['audio_encoder.encoder.pos_conv_embed.conv.weight_g', 'audio_encoder.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing NueASRModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NueASRModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of NueASRModel were not initialized from the model checkpoint at rinna/nue-asr and are newly initialized: ['audio_encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'audio_encoder.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transcribe time: 5.38s\n",
      "['使い古された技術を使った。',\n",
      " '1つの国家に変えてしまおうと。',\n",
      " 'でもな俺しゃべり下手やしな。',\n",
      " 'コミュニケーションも下手で。',\n",
      " 'でも一方で。',\n",
      " 'するともう本当にだんだん体が動かなくなって。',\n",
      " 'それと同時に貧しさや差別。',\n",
      " '学校には机と椅子がありませんでした。',\n",
      " '曖昧な部分があったりとかって結構しますね。',\n",
      " '山に登ったり。']\n",
      "0.21052631578947367\n"
     ]
    }
   ],
   "source": [
    "import nue_asr\n",
    "import time\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "\n",
    "paths2audio_files = ['/root/datadrive/TEDxJP-10K_v1.1/test_dump/Tc2--M7NQrA-00096719-00096990.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/saHZ4bN3h28-00039574-00039725.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/KpRpQsojKgc-00083461-00083799.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/sA4Cj96KMi0-00033229-00033405.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/IBrUxfKQdEA-00014000-00014147.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/cZy6z806Lyg-00100804-00101180.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/Yb04vLKSvxQ-00066642-00067042.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/VhFMEJhnTNk-00035949-00036235.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/d0DrguC0Flc-00060864-00061250.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/0jJLyvNn_to-00036300-00036412.wav']\n",
    "file_id_list = [Path(p).stem for p in paths2audio_files]\n",
    "gt_text = [id2text[file_id] for file_id in file_id_list]\n",
    "\n",
    "\n",
    "model = nue_asr.load_model(\"rinna/nue-asr\")\n",
    "tokenizer = nue_asr.load_tokenizer(\"rinna/nue-asr\")\n",
    "\n",
    "asr_text = []\n",
    "tic = time.perf_counter()\n",
    "for wav_file in paths2audio_files:\n",
    "    result = nue_asr.transcribe(model, tokenizer, wav_file)\n",
    "    asr_text.append(result.text)\n",
    "toc = time.perf_counter() - tic\n",
    "\n",
    "print(f\"transcribe time: {toc:.2f}s\")\n",
    "pprint(asr_text)\n",
    "cer = calculate_cer(gt_text, asr_text)\n",
    "print(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d979615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7417.005312"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([p.numel() for p in model.parameters()]) / 1e6 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "296659a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(model.parameters())).data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f476523a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/workspace/nemo.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f707974686f6e2d6465762d707974686f6e2d6465762d72756e2d396262613164346536313963227d/root/workspace/nemo.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdel\u001b[39;00m pipe\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f707974686f6e2d6465762d707974686f6e2d6465762d72756e2d396262613164346536313963227d/root/workspace/nemo.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdel\u001b[39;00m model\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f707974686f6e2d6465762d707974686f6e2d6465762d72756e2d396262613164346536313963227d/root/workspace/nemo.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "del pipe\n",
    "del model\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf3c66a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 31 00:13:45 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   49C    P3              28W / 170W |  10058MiB / 12288MiB |     40%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8be77666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/root/datadrive/TEDxJP-10K_v1.1/test_dump/Tc2--M7NQrA-00096719-00096990.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/saHZ4bN3h28-00039574-00039725.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/KpRpQsojKgc-00083461-00083799.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/sA4Cj96KMi0-00033229-00033405.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/IBrUxfKQdEA-00014000-00014147.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/cZy6z806Lyg-00100804-00101180.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/Yb04vLKSvxQ-00066642-00067042.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/VhFMEJhnTNk-00035949-00036235.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/d0DrguC0Flc-00060864-00061250.wav', '/root/datadrive/TEDxJP-10K_v1.1/test_dump/0jJLyvNn_to-00036300-00036412.wav']\n"
     ]
    }
   ],
   "source": [
    "print(paths2audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8446c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
